{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "polish-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#import nlp\n",
    "from transformers import LongformerTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mounted-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-watson",
   "metadata": {},
   "source": [
    "# Dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cognitive-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"nq/train_wiki3_na_filtered_qg_t5l35-sqd_filtered.json\", \"r\") as fh:\n",
    "#     train_dataset = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pediatric-cause",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset natural_questions (/home/sri/.cache/huggingface/datasets/natural_questions/default/0.0.2/867dbbaf9137c1b83ecb19f5eb80559e1002ea26e702c6b919cfa81a17a8c531)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"natural_questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-display",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = dataset['train'][1]\n",
    "\n",
    "start_idx, end_idx = example['annotations']['long_answer'][0]['start_token'], example['annotations']['long_answer'][0]['end_token']\n",
    "\n",
    "\" \".join([itm for idx, itm in enumerate(example['document']['tokens']['token'][start_idx:end_idx]) if not example['document']['tokens']['is_html'][start_idx+idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "INPUT_FILE = \"natural-questions/simplified-nq-train.jsonl\"\n",
    "START_TOKEN = 3521\n",
    "END_TOKEN = 3525\n",
    "QAS_ID = 4549465242785278785\n",
    "REMOVE_HTML = True\n",
    "\n",
    "\n",
    "def get_span_from_token_offsets(start_token, end_token, qas_id,\n",
    "                                remove_html):\n",
    "    for obj in f:\n",
    "        if obj[\"example_id\"] != qas_id:\n",
    "            continue\n",
    "        print(\"Found\")\n",
    "        return obj\n",
    "        if remove_html:\n",
    "            answer_span = [\n",
    "                item[\"token\"]\n",
    "                for item in obj[\"document_tokens\"][start_token:end_token]\n",
    "                if not item[\"html_token\"]\n",
    "            ]\n",
    "        else:\n",
    "            answer_span = [\n",
    "                item[\"token\"]\n",
    "                for item in obj[\"document_tokens\"][start_token:end_token]\n",
    "            ]\n",
    "\n",
    "        return \" \".join(answer_span)\n",
    "\n",
    "\n",
    "with jsonlines.open(INPUT_FILE) as f:\n",
    "    result = get_span_from_token_offsets(f, START_TOKEN, END_TOKEN, QAS_ID,\n",
    "                                         REMOVE_HTML)\n",
    "\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in dataset['train']:\n",
    "    if example['id']!='5648415780048191748':\n",
    "        start_idx, end_idx = example['annotations']['long_answer'][0]['start_token'], example['annotations']['long_answer'][0]['end_token']\n",
    "        answer  = \" \".join([itm for idx, itm in enumerate(example['document']['tokens']['token'][start_idx:end_idx]) if not example['document']['tokens']['is_html'][start_idx+idx]])\n",
    "        context = \" \".join([itm for idx, itm in enumerate(example['document']['tokens']['token'][:]) if not example['document']['tokens']['is_html'][idx]])\n",
    "        query = example['question']['text']\n",
    "        input_pairs =[query, context]\n",
    "        encodings = tokenizer.encode_plus(input_pairs, pad_to_max_length=True, max_length=4096)\n",
    "        context_encodings = tokenizer.encode_plus(context)\n",
    "\n",
    "        start_byte_idx = context.find(answer)\n",
    "        end_byte_idx = start_byte_idx + len(answer)-1\n",
    "        #print(start_idx)\n",
    "        start_positions_context = context_encodings.char_to_token(start_byte_idx)\n",
    "        end_positions_context = context_encodings.char_to_token(end_byte_idx)\n",
    "\n",
    "        sep_idx = encodings['input_ids'].index(tokenizer.sep_token_id)\n",
    "        print(len(encodings['input_ids']))\n",
    "        #print(context,\"\\n\\n\" ,answer)\n",
    "        start_positions = start_positions_context + sep_idx + 1\n",
    "        end_positions = end_positions_context + sep_idx + 1\n",
    "\n",
    "        if end_positions > 4096:\n",
    "              start_positions, end_positions = 0, 0\n",
    "\n",
    "                \n",
    "        encodings.update({'start_positions': start_positions,\n",
    "                              'end_positions': end_positions,\n",
    "                              'attention_mask': encodings['attention_mask']})\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "context[start_byte_idx:end_byte_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_byte_idx - start_byte_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spanish-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(example, max_length=1024):\n",
    "    try:\n",
    "    #example = dataset['train'][0]\n",
    "        start_idx, end_idx = example['annotations']['long_answer'][0]['start_token'], example['annotations']['long_answer'][0]['end_token']\n",
    "        answer  = \" \".join([itm for idx, itm in enumerate(example['document']['tokens']['token'][start_idx:end_idx]) if not example['document']['tokens']['is_html'][start_idx+idx]])\n",
    "        context = \" \".join([itm for idx, itm in enumerate(example['document']['tokens']['token'][:]) if not example['document']['tokens']['is_html'][idx]])\n",
    "        query = example['question']['text']\n",
    "        input_pairs =[query, context]\n",
    "        #encodings = tokenizer.encode_plus(input_pairs, pad_to_max_length=True, truncation=True, max_length=max_length)\n",
    "        encodings = tokenizer.encode_plus(input_pairs, padding='max_length', truncation=True, max_length=max_length)\n",
    "        context_encodings = tokenizer.encode_plus(context)\n",
    "\n",
    "        start_byte_idx = context.find(answer)\n",
    "        end_byte_idx = start_byte_idx + len(answer)-1\n",
    "\n",
    "        if start_idx==-1:\n",
    "            #print(start_idx)\n",
    "            encodings.update({'start_positions': 0,\n",
    "                              'end_positions': 0,\n",
    "                              'attention_mask': encodings['attention_mask']})\n",
    "            return encodings\n",
    "        start_positions_context = context_encodings.char_to_token(start_byte_idx)\n",
    "        end_positions_context = context_encodings.char_to_token(end_byte_idx)\n",
    "\n",
    "        sep_idx = encodings['input_ids'].index(tokenizer.sep_token_id)\n",
    "\n",
    "        start_positions = start_positions_context + sep_idx + 1\n",
    "        end_positions = end_positions_context + sep_idx + 1\n",
    "\n",
    "        if end_positions > 4096:\n",
    "              start_positions, end_positions = 0, 0\n",
    "\n",
    "        encodings.update({'start_positions': start_positions,\n",
    "                              'end_positions': end_positions,\n",
    "                              'attention_mask': encodings['attention_mask']})\n",
    "    except Exception as e:\n",
    "        #print(start_idx, example)\n",
    "        print(e, example['id'])\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[8]['annotations']['long_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "broke-course",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6404 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47df181150a242e484fea4331f2ca056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307373 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = dataset['train']\n",
    "train_dataset = train_dataset.map(convert_to_features)\n",
    "\n",
    "torch.save(train_dataset, 'data/train_data_full.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "welcome-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "\n",
    "torch.save(train_dataset, 'data/train_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "choice-scenario",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1bb72203594d59a1a141537bd8eca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7830 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_dataset = dataset['validation']\n",
    "valid_dataset = valid_dataset.map(convert_to_features, load_from_cache_file=False)\n",
    "\n",
    "torch.save(valid_dataset, 'data/valid_data_full.pt')\n",
    "\n",
    "columns = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "valid_dataset.set_format(type='torch', columns=columns)\n",
    "\n",
    "torch.save(valid_dataset, 'data/valid_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'data/train_data_full.pt')\n",
    "#torch.save(valid_dataset, 'data/valid_data_full.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "#valid_dataset.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "example['annotations']['long_answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cach the dataset, so we can load it directly for training\n",
    "\n",
    "torch.save(train_dataset, 'data/train_data.pt')\n",
    "#torch.save(valid_dataset, 'data/valid_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = torch.load(\"data/train_data.pt\")\n",
    "# valid_dataset = torch.load(\"data/valid_data.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-victoria",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import LongformerForQuestionAnswering, LongformerTokenizerFast, EvalPrediction\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    DataCollator,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    model/config/tokenizer arguments\n",
    "    \"\"\"\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from hugginface.co/models\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    model input arguments\n",
    "    \"\"\"\n",
    "    train_file_path: Optional[str] = field(\n",
    "        default='data/train_data.pt',\n",
    "        metadata={\"help\": \"Path for cached train dataset\"}\n",
    "    )\n",
    "    valid_file_path: Optional[str] = field(\n",
    "        default='data/valid_data.pt',\n",
    "        metadata={\"help\": \"Path for cached valid dataset\"}\n",
    "    )\n",
    "    max_len: Optional[str] = field(\n",
    "        default=4096,\n",
    "        metadata={\"help\": \"Max input length for the source text\"}\n",
    "    )\n",
    "        \n",
    "\n",
    "class DummyDataCollator():\n",
    "    def __call__(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Take a list of samples from a Dataset and collate them into a batch.\n",
    "        Returns:\n",
    "            A dictionary of tensors\n",
    "        \"\"\"\n",
    "        input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "        start_positions = torch.stack([example['start_positions'] for example in batch])\n",
    "        end_positions = torch.stack([example['end_positions'] for example in batch])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'start_positions': start_positions, \n",
    "            'end_positions': end_positions,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    \n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath('args.json'))\n",
    "    \n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "        \n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        training_args.local_rank,\n",
    "        training_args.device,\n",
    "        training_args.n_gpu,\n",
    "        bool(training_args.local_rank != -1),\n",
    "        training_args.fp16,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "    \n",
    "    tokenizer = LongformerTokenizerFast.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    model = LongformerForQuestionAnswering.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    \n",
    "    # Get datasets\n",
    "    print('loading data')\n",
    "    train_dataset  = torch.load(data_args.train_file_path)\n",
    "    valid_dataset = torch.load(data_args.valid_file_path)\n",
    "    print('loading done')\n",
    "    \n",
    "    \n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=DummyDataCollator(),\n",
    " #       prediction_loss_only=True,\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        trainer.train(\n",
    "            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "        )\n",
    "        trainer.save_model()\n",
    "        # For convenience, we also re-save the tokenizer to the same directory,\n",
    "        # so that you can share your model easily on huggingface.co/models =)\n",
    "        if trainer.is_world_master():\n",
    "            tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if training_args.do_eval and training_args.local_rank in [-1, 0]:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        eval_output = trainer.evaluate()\n",
    "\n",
    "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(eval_output.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(eval_output[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(eval_output[key])))\n",
    "    \n",
    "        results.update(eval_output)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "args_dict = {\n",
    "  \"n_gpu\": 2,\n",
    "  \"model_name_or_path\": 'allenai/longformer-base-4096',\n",
    "  \"max_len\": 4096 ,\n",
    "  \"output_dir\": './models',\n",
    "  \"overwrite_output_dir\": True,\n",
    "  \"per_gpu_train_batch_size\": 8,\n",
    "  \"per_gpu_eval_batch_size\": 8,\n",
    "  \"gradient_accumulation_steps\": 16,\n",
    "  \"learning_rate\": 1e-4,\n",
    "  \"num_train_epochs\": 3,\n",
    "  \"do_train\": True\n",
    "}\n",
    "\n",
    "with open('args.json', 'w') as f:\n",
    "    json.dump(args_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-forest",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for d in  train_dataset:\n",
    "    print (d['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-inspection",
   "metadata": {},
   "source": [
    "# args testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "HfArgumentParser,\n",
    "TrainingArguments,\n",
    ")\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_train_args():\n",
    "    parser = argparse.ArgumentParser('Train spanBert model on NQ')\n",
    "\n",
    "    parser.add_argument('--name',\n",
    "                        type=str,\n",
    "                        default='SpanBert',\n",
    "                        help='Train Name which we use to identify the training. Need not be unique.')\n",
    "    parser.add_argument('--train_file_path',\n",
    "                        type=str,\n",
    "                        default='data/train_data.pt',\n",
    "                        help='Path for cached train dataset')\n",
    "    parser.add_argument('--valid_file_path',\n",
    "                        type=str,\n",
    "                        default='data/valid_data.pt',\n",
    "                        help='Path for cached valid dataset')\n",
    "    parser.add_argument('--max_len',\n",
    "                        type=str,\n",
    "                        default=4096,\n",
    "                        help='Max input length for the source text.')\n",
    "    parser.add_argument('--num_epochs',\n",
    "                        type=int,\n",
    "                        default=3,\n",
    "                        help='Number of epochs for which to train. Negative means forever.')\n",
    "    parser.add_argument('--n_gpu',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help='Number of GPU to train the model on.')\n",
    "    parser.add_argument('--save_dir',\n",
    "                        type=str,\n",
    "                        default='./save',\n",
    "                        help='Directory to store the model in (If already exists, use --overwrite_output_dir).')\n",
    "    parser.add_argument('--overwrite_output_dir',\n",
    "                        type=bool,\n",
    "                        default=True,\n",
    "                        help='Overwrite existing model directory')\n",
    "    parser.add_argument('--per_device_train_batch_size',\n",
    "                        type=int,\n",
    "                        default=8)\n",
    "    parser.add_argument('--per_gpu_eval_batch_size',\n",
    "                        type=int,\n",
    "                        default=8)\n",
    "    parser.add_argument('--gradient_accumulation_steps',\n",
    "                        type=int,\n",
    "                        default=16)\n",
    "    parser.add_argument('--learning_rate',\n",
    "                        type=float,\n",
    "                        default=1e-4,\n",
    "                        help='Learning rate for the model')\n",
    "    parser.add_argument('--num_train_epochs',\n",
    "                        type=int,\n",
    "                        default=3,\n",
    "                        help='Number of epochs for which to train. Negative means forever.')\n",
    "    parser.add_argument('--do_train',\n",
    "                        type=bool,\n",
    "                        default=True,\n",
    "                        help='Whether to train the model.')\n",
    "    parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=42,\n",
    "                        help='Seed to reproduce.')\n",
    "    parser.add_argument('--do_eval',\n",
    "                        type=bool,\n",
    "                        default=True,\n",
    "                        help='Whether to evaluate the model after training.')\n",
    "    parser.add_argument('--local_rank',\n",
    "                        type=int,\n",
    "                        default=-1)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from args import get_train_args\n",
    "parser = get_train_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-parallel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-citizenship",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
