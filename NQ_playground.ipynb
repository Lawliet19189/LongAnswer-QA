{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "oriental-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#import nlp\n",
    "from transformers import LongformerTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electrical-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-easter",
   "metadata": {},
   "source": [
    "# Dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prompt-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"nq/train_wiki3_na_filtered_qg_t5l35-sqd_filtered.json\", \"r\") as fh:\n",
    "#     train_dataset = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "designing-portable",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset natural_questions (/home/sri/.cache/huggingface/datasets/natural_questions/default/0.0.2/867dbbaf9137c1b83ecb19f5eb80559e1002ea26e702c6b919cfa81a17a8c531)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"natural_questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "suspected-office",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ctrl + Fn + F11 or Fn + B or Fn + Ctrl + B on certain Lenovo laptops and certain Dell laptops . Fn + Esc on Samsung . Ctrl + Fn + â‡§ Shift on certain HP laptops .'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][1]\n",
    "\n",
    "start_idx, end_idx = example['annotations']['long_answer'][0]['start_token'], example['annotations']['long_answer'][0]['end_token']\n",
    "\n",
    "\" \".join([itm for idx, itm in enumerate(example['document']['tokens']['token'][start_idx:end_idx]) if not example['document']['tokens']['is_html'][start_idx+idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mathematical-thanks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "INPUT_FILE = \"natural-questions/simplified-nq-train.jsonl\"\n",
    "START_TOKEN = 3521\n",
    "END_TOKEN = 3525\n",
    "QAS_ID = 4549465242785278785\n",
    "REMOVE_HTML = True\n",
    "\n",
    "\n",
    "def get_span_from_token_offsets(start_token, end_token, qas_id,\n",
    "                                remove_html):\n",
    "    for obj in f:\n",
    "        if obj[\"example_id\"] != qas_id:\n",
    "            continue\n",
    "        print(\"Found\")\n",
    "        return obj\n",
    "        if remove_html:\n",
    "            answer_span = [\n",
    "                item[\"token\"]\n",
    "                for item in obj[\"document_tokens\"][start_token:end_token]\n",
    "                if not item[\"html_token\"]\n",
    "            ]\n",
    "        else:\n",
    "            answer_span = [\n",
    "                item[\"token\"]\n",
    "                for item in obj[\"document_tokens\"][start_token:end_token]\n",
    "            ]\n",
    "\n",
    "        return \" \".join(answer_span)\n",
    "\n",
    "\n",
    "with jsonlines.open(INPUT_FILE) as f:\n",
    "    result = get_span_from_token_offsets(f, START_TOKEN, END_TOKEN, QAS_ID,\n",
    "                                         REMOVE_HTML)\n",
    "\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "coordinate-equivalent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "4096\n",
      "4096\n",
      "4096\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "can't convert negative int to unsigned",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-246-4ef2c7d1b4f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print(start_idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mstart_positions_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_encodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_byte_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mend_positions_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_encodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_byte_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msep_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mchar_to_token\u001b[0;34m(self, batch_or_char_index, char_index, sequence_index)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mchar_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_or_char_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     def word_to_chars(\n",
      "\u001b[0;31mOverflowError\u001b[0m: can't convert negative int to unsigned"
     ]
    }
   ],
   "source": [
    "for example in dataset['train']:\n",
    "    if example['id']!='5648415780048191748':\n",
    "        start_idx, end_idx = example['annotations']['long_answer'][0]['start_token'], example['annotations']['long_answer'][0]['end_token']\n",
    "        answer  = \" \".join([itm for idx, itm in enumerate(example['document']['tokens']['token'][start_idx:end_idx]) if not example['document']['tokens']['is_html'][start_idx+idx]])\n",
    "        context = \" \".join([itm for idx, itm in enumerate(example['document']['tokens']['token'][:]) if not example['document']['tokens']['is_html'][idx]])\n",
    "        query = example['question']['text']\n",
    "        input_pairs =[query, context]\n",
    "        encodings = tokenizer.encode_plus(input_pairs, pad_to_max_length=True, max_length=4096)\n",
    "        context_encodings = tokenizer.encode_plus(context)\n",
    "\n",
    "        start_byte_idx = context.find(answer)\n",
    "        end_byte_idx = start_byte_idx + len(answer)-1\n",
    "        #print(start_idx)\n",
    "        start_positions_context = context_encodings.char_to_token(start_byte_idx)\n",
    "        end_positions_context = context_encodings.char_to_token(end_byte_idx)\n",
    "\n",
    "        sep_idx = encodings['input_ids'].index(tokenizer.sep_token_id)\n",
    "        print(len(encodings['input_ids']))\n",
    "        #print(context,\"\\n\\n\" ,answer)\n",
    "        start_positions = start_positions_context + sep_idx + 1\n",
    "        end_positions = end_positions_context + sep_idx + 1\n",
    "\n",
    "        if end_positions > 4096:\n",
    "              start_positions, end_positions = 0, 0\n",
    "\n",
    "                \n",
    "        encodings.update({'start_positions': start_positions,\n",
    "                              'end_positions': end_positions,\n",
    "                              'attention_mask': encodings['attention_mask']})\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "tamil-april",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Over the next several months , pilots were trained , equipment was adapted , and intelligence was collected . Despite these preparations , Emperor Hirohito did not approve the attack plan until November 5 , after the third of four Imperial Conferences called to consider the matter . Final authorization was not given by the emperor until December 1 , after a majority of Japanese leaders advised him the `` Hull Note '' would `` destroy the fruits of the China incident , endanger Manchukuo and undermine Japanese control of Korea . ''\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[start_byte_idx:end_byte_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "manual-absence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_byte_idx - start_byte_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "competent-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(example):\n",
    "    try:\n",
    "    #example = dataset['train'][0]\n",
    "        start_idx, end_idx = example['annotations']['long_answer'][0]['start_token'], example['annotations']['long_answer'][0]['end_token']\n",
    "        answer  = \" \".join([itm for idx, itm in enumerate(example['document']['tokens']['token'][start_idx:end_idx]) if not example['document']['tokens']['is_html'][start_idx+idx]])\n",
    "        context = \" \".join([itm for idx, itm in enumerate(example['document']['tokens']['token'][:]) if not example['document']['tokens']['is_html'][idx]])\n",
    "        query = example['question']['text']\n",
    "        input_pairs =[query, context]\n",
    "        encodings = tokenizer.encode_plus(input_pairs, pad_to_max_length=True, max_length=4096)\n",
    "        context_encodings = tokenizer.encode_plus(context)\n",
    "\n",
    "        start_byte_idx = context.find(answer)\n",
    "        end_byte_idx = start_byte_idx + len(answer)-1\n",
    "\n",
    "        if start_idx==-1:\n",
    "            #print(start_idx)\n",
    "            encodings.update({'start_positions': 0,\n",
    "                              'end_positions': 0,\n",
    "                              'attention_mask': encodings['attention_mask']})\n",
    "            return encodings\n",
    "        start_positions_context = context_encodings.char_to_token(start_byte_idx)\n",
    "        end_positions_context = context_encodings.char_to_token(end_byte_idx)\n",
    "\n",
    "        sep_idx = encodings['input_ids'].index(tokenizer.sep_token_id)\n",
    "\n",
    "        start_positions = start_positions_context + sep_idx + 1\n",
    "        end_positions = end_positions_context + sep_idx + 1\n",
    "\n",
    "        if end_positions > 4096:\n",
    "              start_positions, end_positions = 0, 0\n",
    "\n",
    "        encodings.update({'start_positions': start_positions,\n",
    "                              'end_positions': end_positions,\n",
    "                              'attention_mask': encodings['attention_mask']})\n",
    "    except Exception as e:\n",
    "        #print(start_idx, example)\n",
    "        print(example['id'])\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "consecutive-runner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'end_byte': 112155,\n",
       "  'end_token': 2648,\n",
       "  'start_byte': 111251,\n",
       "  'start_token': 2553}]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[8]['annotations']['long_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "developing-novelty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66904ade6174431d86cac98586e22542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=307373.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train']\n",
    "train_dataset = train_dataset.map(convert_to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "fatal-prototype",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5226efca0c4d1fbf7dbc409d15ebfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7830.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = dataset['validation']\n",
    "valid_dataset = valid_dataset.map(convert_to_features, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "express-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'data/train_data_full.pt')\n",
    "torch.save(valid_dataset, 'data/valid_data_full.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "failing-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "valid_dataset.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "distinguished-hamburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307373, 7830)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "worthy-folder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end_byte': 73287, 'end_token': 790, 'start_byte': 69526, 'start_token': 374}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['annotations']['long_answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "sixth-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cach the dataset, so we can load it directly for training\n",
    "\n",
    "torch.save(train_dataset, 'data/train_data.pt')\n",
    "torch.save(valid_dataset, 'data/valid_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "green-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = torch.load(\"data/train_data.pt\")\n",
    "# valid_dataset = torch.load(\"data/valid_data.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-dominant",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accomplished-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import LongformerForQuestionAnswering, LongformerTokenizerFast, EvalPrediction\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    DataCollator,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "velvet-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    model/config/tokenizer arguments\n",
    "    \"\"\"\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from hugginface.co/models\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    model input arguments\n",
    "    \"\"\"\n",
    "    train_file_path: Optional[str] = field(\n",
    "        default='data/train_data.pt',\n",
    "        metadata={\"help\": \"Path for cached train dataset\"}\n",
    "    )\n",
    "    valid_file_path: Optional[str] = field(\n",
    "        default='data/valid_data.pt',\n",
    "        metadata={\"help\": \"Path for cached valid dataset\"}\n",
    "    )\n",
    "    max_len: Optional[str] = field(\n",
    "        default=4096,\n",
    "        metadata={\"help\": \"Max input length for the source text\"}\n",
    "    )\n",
    "        \n",
    "\n",
    "class DummyDataCollator():\n",
    "    def __call__(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Take a list of samples from a Dataset and collate them into a batch.\n",
    "        Returns:\n",
    "            A dictionary of tensors\n",
    "        \"\"\"\n",
    "        input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "        start_positions = torch.stack([example['start_positions'] for example in batch])\n",
    "        end_positions = torch.stack([example['end_positions'] for example in batch])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'start_positions': start_positions, \n",
    "            'end_positions': end_positions,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incident-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    \n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath('args.json'))\n",
    "    \n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "        \n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        training_args.local_rank,\n",
    "        training_args.device,\n",
    "        training_args.n_gpu,\n",
    "        bool(training_args.local_rank != -1),\n",
    "        training_args.fp16,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "    \n",
    "    tokenizer = LongformerTokenizerFast.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    model = LongformerForQuestionAnswering.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    \n",
    "    # Get datasets\n",
    "    print('loading data')\n",
    "    train_dataset  = torch.load(data_args.train_file_path)\n",
    "    valid_dataset = torch.load(data_args.valid_file_path)\n",
    "    print('loading done')\n",
    "    \n",
    "    \n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=DummyDataCollator(),\n",
    " #       prediction_loss_only=True,\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        trainer.train(\n",
    "            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "        )\n",
    "        trainer.save_model()\n",
    "        # For convenience, we also re-save the tokenizer to the same directory,\n",
    "        # so that you can share your model easily on huggingface.co/models =)\n",
    "        if trainer.is_world_master():\n",
    "            tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if training_args.do_eval and training_args.local_rank in [-1, 0]:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        eval_output = trainer.evaluate()\n",
    "\n",
    "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(eval_output.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(eval_output[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(eval_output[key])))\n",
    "    \n",
    "        results.update(eval_output)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mighty-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "args_dict = {\n",
    "  \"n_gpu\": 2,\n",
    "  \"model_name_or_path\": 'allenai/longformer-base-4096',\n",
    "  \"max_len\": 4096 ,\n",
    "  \"output_dir\": './models',\n",
    "  \"overwrite_output_dir\": True,\n",
    "  \"per_gpu_train_batch_size\": 8,\n",
    "  \"per_gpu_eval_batch_size\": 8,\n",
    "  \"gradient_accumulation_steps\": 16,\n",
    "  \"learning_rate\": 1e-4,\n",
    "  \"num_train_epochs\": 3,\n",
    "  \"do_train\": True\n",
    "}\n",
    "\n",
    "with open('args.json', 'w') as f:\n",
    "    json.dump(args_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "civic-application",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/02/2021 10:11:45 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "04/02/2021 10:11:45 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./models, overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=16, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr02_10-11-45_pop-os, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./models, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=2)\n",
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForQuestionAnswering were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "loading done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1988, in forward\n    outputs = self.longformer(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1668, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1278, in forward\n    layer_outputs = layer_module(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1204, in forward\n    self_attn_outputs = self.attention(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1140, in forward\n    self_outputs = self.self(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 666, in forward\n    attn_output = self._compute_attn_output_with_global_indices(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 987, in _compute_attn_output_with_global_indices\n    attn_output_without_global = self._sliding_chunks_matmul_attn_probs_value(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 874, in _sliding_chunks_matmul_attn_probs_value\n    chunked_attn_probs = attn_probs.transpose(1, 2).reshape(\nRuntimeError: CUDA out of memory. Tried to allocate 194.00 MiB (GPU 0; 10.91 GiB total capacity; 9.16 GiB already allocated; 193.56 MiB free; 9.65 GiB reserved in total by PyTorch)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-1e951f23b7b7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         trainer.train(\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1475\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1476\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1988, in forward\n    outputs = self.longformer(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1668, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1278, in forward\n    layer_outputs = layer_module(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1204, in forward\n    self_attn_outputs = self.attention(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 1140, in forward\n    self_outputs = self.self(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 666, in forward\n    attn_output = self._compute_attn_output_with_global_indices(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 987, in _compute_attn_output_with_global_indices\n    attn_output_without_global = self._sliding_chunks_matmul_attn_probs_value(\n  File \"/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\", line 874, in _sliding_chunks_matmul_attn_probs_value\n    chunked_attn_probs = attn_probs.transpose(1, 2).reshape(\nRuntimeError: CUDA out of memory. Tried to allocate 194.00 MiB (GPU 0; 10.91 GiB total capacity; 9.16 GiB already allocated; 193.56 MiB free; 9.65 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "resident-pastor",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6423])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8298])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7672])\n",
      "torch.Size([7985])\n",
      "torch.Size([21720])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([37835])\n",
      "torch.Size([16348])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5981])\n",
      "torch.Size([5932])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6693])\n",
      "torch.Size([5397])\n",
      "torch.Size([5967])\n",
      "torch.Size([8351])\n",
      "torch.Size([17922])\n",
      "torch.Size([31421])\n",
      "torch.Size([6096])\n",
      "torch.Size([17593])\n",
      "torch.Size([7101])\n",
      "torch.Size([4558])\n",
      "torch.Size([12074])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4316])\n",
      "torch.Size([4096])\n",
      "torch.Size([19739])\n",
      "torch.Size([12279])\n",
      "torch.Size([14599])\n",
      "torch.Size([5210])\n",
      "torch.Size([23179])\n",
      "torch.Size([4096])\n",
      "torch.Size([4259])\n",
      "torch.Size([5160])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5504])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([34150])\n",
      "torch.Size([31218])\n",
      "torch.Size([9512])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([13786])\n",
      "torch.Size([17891])\n",
      "torch.Size([4096])\n",
      "torch.Size([5309])\n",
      "torch.Size([4096])\n",
      "torch.Size([4900])\n",
      "torch.Size([4096])\n",
      "torch.Size([15116])\n",
      "torch.Size([11753])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4660])\n",
      "torch.Size([4096])\n",
      "torch.Size([11140])\n",
      "torch.Size([21258])\n",
      "torch.Size([22340])\n",
      "torch.Size([4501])\n",
      "torch.Size([4096])\n",
      "torch.Size([14465])\n",
      "torch.Size([8156])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([11836])\n",
      "torch.Size([4096])\n",
      "torch.Size([9826])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6810])\n",
      "torch.Size([8829])\n",
      "torch.Size([5801])\n",
      "torch.Size([18435])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([34160])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([14438])\n",
      "torch.Size([28871])\n",
      "torch.Size([14899])\n",
      "torch.Size([4096])\n",
      "torch.Size([16089])\n",
      "torch.Size([5786])\n",
      "torch.Size([6149])\n",
      "torch.Size([14092])\n",
      "torch.Size([31528])\n",
      "torch.Size([4096])\n",
      "torch.Size([14175])\n",
      "torch.Size([4096])\n",
      "torch.Size([11717])\n",
      "torch.Size([23161])\n",
      "torch.Size([4096])\n",
      "torch.Size([4927])\n",
      "torch.Size([4374])\n",
      "torch.Size([4096])\n",
      "torch.Size([12976])\n",
      "torch.Size([18702])\n",
      "torch.Size([34434])\n",
      "torch.Size([4629])\n",
      "torch.Size([11205])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([13581])\n",
      "torch.Size([21018])\n",
      "torch.Size([11595])\n",
      "torch.Size([4096])\n",
      "torch.Size([8187])\n",
      "torch.Size([4096])\n",
      "torch.Size([7079])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6034])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5074])\n",
      "torch.Size([4096])\n",
      "torch.Size([24237])\n",
      "torch.Size([10839])\n",
      "torch.Size([4096])\n",
      "torch.Size([5620])\n",
      "torch.Size([4721])\n",
      "torch.Size([7301])\n",
      "torch.Size([6647])\n",
      "torch.Size([12940])\n",
      "torch.Size([14197])\n",
      "torch.Size([12236])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8313])\n",
      "torch.Size([4096])\n",
      "torch.Size([5197])\n",
      "torch.Size([21665])\n",
      "torch.Size([4096])\n",
      "torch.Size([13246])\n",
      "torch.Size([8365])\n",
      "torch.Size([9579])\n",
      "torch.Size([4096])\n",
      "torch.Size([14071])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([22027])\n",
      "torch.Size([4284])\n",
      "torch.Size([23153])\n",
      "torch.Size([4096])\n",
      "torch.Size([4697])\n",
      "torch.Size([5509])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([12414])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7594])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([14691])\n",
      "torch.Size([5546])\n",
      "torch.Size([4096])\n",
      "torch.Size([19249])\n",
      "torch.Size([5830])\n",
      "torch.Size([5812])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6448])\n",
      "torch.Size([5433])\n",
      "torch.Size([4096])\n",
      "torch.Size([4878])\n",
      "torch.Size([4096])\n",
      "torch.Size([30588])\n",
      "torch.Size([18250])\n",
      "torch.Size([22183])\n",
      "torch.Size([6719])\n",
      "torch.Size([4096])\n",
      "torch.Size([12002])\n",
      "torch.Size([8862])\n",
      "torch.Size([8959])\n",
      "torch.Size([4189])\n",
      "torch.Size([4286])\n",
      "torch.Size([25697])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6403])\n",
      "torch.Size([30854])\n",
      "torch.Size([4096])\n",
      "torch.Size([28202])\n",
      "torch.Size([4307])\n",
      "torch.Size([4562])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([11426])\n",
      "torch.Size([4096])\n",
      "torch.Size([15958])\n",
      "torch.Size([13661])\n",
      "torch.Size([8159])\n",
      "torch.Size([4222])\n",
      "torch.Size([26643])\n",
      "torch.Size([4096])\n",
      "torch.Size([25009])\n",
      "torch.Size([4096])\n",
      "torch.Size([14915])\n",
      "torch.Size([8422])\n",
      "torch.Size([8506])\n",
      "torch.Size([8895])\n",
      "torch.Size([22947])\n",
      "torch.Size([4096])\n",
      "torch.Size([15865])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([21916])\n",
      "torch.Size([4661])\n",
      "torch.Size([9451])\n",
      "torch.Size([4096])\n",
      "torch.Size([25496])\n",
      "torch.Size([4096])\n",
      "torch.Size([25465])\n",
      "torch.Size([20023])\n",
      "torch.Size([4096])\n",
      "torch.Size([14365])\n",
      "torch.Size([7576])\n",
      "torch.Size([30580])\n",
      "torch.Size([7279])\n",
      "torch.Size([4096])\n",
      "torch.Size([17414])\n",
      "torch.Size([14060])\n",
      "torch.Size([4096])\n",
      "torch.Size([16715])\n",
      "torch.Size([4096])\n",
      "torch.Size([12665])\n",
      "torch.Size([5507])\n",
      "torch.Size([10664])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8826])\n",
      "torch.Size([5302])\n",
      "torch.Size([4096])\n",
      "torch.Size([5573])\n",
      "torch.Size([8319])\n",
      "torch.Size([9747])\n",
      "torch.Size([4957])\n",
      "torch.Size([36166])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4589])\n",
      "torch.Size([7170])\n",
      "torch.Size([4348])\n",
      "torch.Size([29254])\n",
      "torch.Size([20942])\n",
      "torch.Size([4096])\n",
      "torch.Size([29592])\n",
      "torch.Size([16911])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4498])\n",
      "torch.Size([4096])\n",
      "torch.Size([4638])\n",
      "torch.Size([5455])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7731])\n",
      "torch.Size([4096])\n",
      "torch.Size([5157])\n",
      "torch.Size([4096])\n",
      "torch.Size([9281])\n",
      "torch.Size([4096])\n",
      "torch.Size([14291])\n",
      "torch.Size([8216])\n",
      "torch.Size([8675])\n",
      "torch.Size([26984])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4860])\n",
      "torch.Size([4096])\n",
      "torch.Size([12711])\n",
      "torch.Size([15171])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7619])\n",
      "torch.Size([6791])\n",
      "torch.Size([5856])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([14551])\n",
      "torch.Size([10221])\n",
      "torch.Size([5473])\n",
      "torch.Size([4096])\n",
      "torch.Size([5075])\n",
      "torch.Size([12640])\n",
      "torch.Size([4441])\n",
      "torch.Size([12909])\n",
      "torch.Size([16274])\n",
      "torch.Size([33567])\n",
      "torch.Size([4096])\n",
      "torch.Size([9762])\n",
      "torch.Size([4096])\n",
      "torch.Size([5455])\n",
      "torch.Size([4096])\n",
      "torch.Size([14526])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4925])\n",
      "torch.Size([4096])\n",
      "torch.Size([9408])\n",
      "torch.Size([12640])\n",
      "torch.Size([7390])\n",
      "torch.Size([11657])\n",
      "torch.Size([4096])\n",
      "torch.Size([4758])\n",
      "torch.Size([7120])\n",
      "torch.Size([12045])\n",
      "torch.Size([4096])\n",
      "torch.Size([6093])\n",
      "torch.Size([12530])\n",
      "torch.Size([4913])\n",
      "torch.Size([8353])\n",
      "torch.Size([4096])\n",
      "torch.Size([18986])\n",
      "torch.Size([4282])\n",
      "torch.Size([4096])\n",
      "torch.Size([17701])\n",
      "torch.Size([4096])\n",
      "torch.Size([17347])\n",
      "torch.Size([6287])\n",
      "torch.Size([15448])\n",
      "torch.Size([4096])\n",
      "torch.Size([4243])\n",
      "torch.Size([4096])\n",
      "torch.Size([6233])\n",
      "torch.Size([5254])\n",
      "torch.Size([13346])\n",
      "torch.Size([8165])\n",
      "torch.Size([15893])\n",
      "torch.Size([4885])\n",
      "torch.Size([8354])\n",
      "torch.Size([6542])\n",
      "torch.Size([9800])\n",
      "torch.Size([5476])\n",
      "torch.Size([4096])\n",
      "torch.Size([8332])\n",
      "torch.Size([8643])\n",
      "torch.Size([29599])\n",
      "torch.Size([4096])\n",
      "torch.Size([4901])\n",
      "torch.Size([6241])\n",
      "torch.Size([7467])\n",
      "torch.Size([4096])\n",
      "torch.Size([12322])\n",
      "torch.Size([19958])\n",
      "torch.Size([4096])\n",
      "torch.Size([9038])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([18839])\n",
      "torch.Size([4096])\n",
      "torch.Size([8059])\n",
      "torch.Size([10816])\n",
      "torch.Size([19178])\n",
      "torch.Size([16104])\n",
      "torch.Size([13921])\n",
      "torch.Size([4096])\n",
      "torch.Size([5910])\n",
      "torch.Size([4096])\n",
      "torch.Size([5374])\n",
      "torch.Size([15600])\n",
      "torch.Size([4096])\n",
      "torch.Size([21851])\n",
      "torch.Size([20227])\n",
      "torch.Size([33869])\n",
      "torch.Size([12317])\n",
      "torch.Size([12205])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6170])\n",
      "torch.Size([4096])\n",
      "torch.Size([13119])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6597])\n",
      "torch.Size([4303])\n",
      "torch.Size([7703])\n",
      "torch.Size([4096])\n",
      "torch.Size([5675])\n",
      "torch.Size([12842])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([12178])\n",
      "torch.Size([6420])\n",
      "torch.Size([5314])\n",
      "torch.Size([4823])\n",
      "torch.Size([6898])\n",
      "torch.Size([4096])\n",
      "torch.Size([10050])\n",
      "torch.Size([12294])\n",
      "torch.Size([4096])\n",
      "torch.Size([8461])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4452])\n",
      "torch.Size([7062])\n",
      "torch.Size([24550])\n",
      "torch.Size([8020])\n",
      "torch.Size([4096])\n",
      "torch.Size([25511])\n",
      "torch.Size([7254])\n",
      "torch.Size([6954])\n",
      "torch.Size([9218])\n",
      "torch.Size([14870])\n",
      "torch.Size([9006])\n",
      "torch.Size([15138])\n",
      "torch.Size([11578])\n",
      "torch.Size([5871])\n",
      "torch.Size([7348])\n",
      "torch.Size([4096])\n",
      "torch.Size([4537])\n",
      "torch.Size([11123])\n",
      "torch.Size([10294])\n",
      "torch.Size([4410])\n",
      "torch.Size([4096])\n",
      "torch.Size([19870])\n",
      "torch.Size([4096])\n",
      "torch.Size([47203])\n",
      "torch.Size([5691])\n",
      "torch.Size([4096])\n",
      "torch.Size([6392])\n",
      "torch.Size([4769])\n",
      "torch.Size([4096])\n",
      "torch.Size([10800])\n",
      "torch.Size([12785])\n",
      "torch.Size([4829])\n",
      "torch.Size([4096])\n",
      "torch.Size([13331])\n",
      "torch.Size([27151])\n",
      "torch.Size([10570])\n",
      "torch.Size([14036])\n",
      "torch.Size([4096])\n",
      "torch.Size([10000])\n",
      "torch.Size([12979])\n",
      "torch.Size([18351])\n",
      "torch.Size([7550])\n",
      "torch.Size([6927])\n",
      "torch.Size([12240])\n",
      "torch.Size([5879])\n",
      "torch.Size([15770])\n",
      "torch.Size([17592])\n",
      "torch.Size([20432])\n",
      "torch.Size([4096])\n",
      "torch.Size([6985])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6015])\n",
      "torch.Size([7522])\n",
      "torch.Size([8374])\n",
      "torch.Size([14552])\n",
      "torch.Size([4856])\n",
      "torch.Size([9548])\n",
      "torch.Size([4198])\n",
      "torch.Size([8560])\n",
      "torch.Size([5977])\n",
      "torch.Size([6916])\n",
      "torch.Size([10609])\n",
      "torch.Size([10041])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([14026])\n",
      "torch.Size([4953])\n",
      "torch.Size([10307])\n",
      "torch.Size([4096])\n",
      "torch.Size([5172])\n",
      "torch.Size([4096])\n",
      "torch.Size([12079])\n",
      "torch.Size([4096])\n",
      "torch.Size([6360])\n",
      "torch.Size([8871])\n",
      "torch.Size([4096])\n",
      "torch.Size([26807])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8762])\n",
      "torch.Size([4096])\n",
      "torch.Size([19598])\n",
      "torch.Size([4096])\n",
      "torch.Size([8736])\n",
      "torch.Size([6207])\n",
      "torch.Size([4170])\n",
      "torch.Size([22134])\n",
      "torch.Size([7396])\n",
      "torch.Size([31228])\n",
      "torch.Size([17116])\n",
      "torch.Size([4096])\n",
      "torch.Size([7142])\n",
      "torch.Size([4296])\n",
      "torch.Size([10380])\n",
      "torch.Size([4096])\n",
      "torch.Size([5748])\n",
      "torch.Size([4096])\n",
      "torch.Size([13867])\n",
      "torch.Size([4264])\n",
      "torch.Size([24592])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([28938])\n",
      "torch.Size([19077])\n",
      "torch.Size([7543])\n",
      "torch.Size([6555])\n",
      "torch.Size([30848])\n",
      "torch.Size([4096])\n",
      "torch.Size([4376])\n",
      "torch.Size([14147])\n",
      "torch.Size([6368])\n",
      "torch.Size([18565])\n",
      "torch.Size([4096])\n",
      "torch.Size([8645])\n",
      "torch.Size([17636])\n",
      "torch.Size([17572])\n",
      "torch.Size([5998])\n",
      "torch.Size([19705])\n",
      "torch.Size([7587])\n",
      "torch.Size([10231])\n",
      "torch.Size([4096])\n",
      "torch.Size([11782])\n",
      "torch.Size([4096])\n",
      "torch.Size([8535])\n",
      "torch.Size([58376])\n",
      "torch.Size([8398])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([28366])\n",
      "torch.Size([5458])\n",
      "torch.Size([7704])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6803])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([9959])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([10579])\n",
      "torch.Size([11425])\n",
      "torch.Size([6775])\n",
      "torch.Size([30744])\n",
      "torch.Size([4096])\n",
      "torch.Size([6129])\n",
      "torch.Size([4096])\n",
      "torch.Size([6460])\n",
      "torch.Size([12840])\n",
      "torch.Size([4096])\n",
      "torch.Size([11864])\n",
      "torch.Size([16820])\n",
      "torch.Size([5078])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([26917])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([16790])\n",
      "torch.Size([4096])\n",
      "torch.Size([11809])\n",
      "torch.Size([4096])\n",
      "torch.Size([8156])\n",
      "torch.Size([6682])\n",
      "torch.Size([4096])\n",
      "torch.Size([11228])\n",
      "torch.Size([25421])\n",
      "torch.Size([7629])\n",
      "torch.Size([4096])\n",
      "torch.Size([12147])\n",
      "torch.Size([8499])\n",
      "torch.Size([4096])\n",
      "torch.Size([5604])\n",
      "torch.Size([24107])\n",
      "torch.Size([10566])\n",
      "torch.Size([24556])\n",
      "torch.Size([4096])\n",
      "torch.Size([10654])\n",
      "torch.Size([10780])\n",
      "torch.Size([16946])\n",
      "torch.Size([30163])\n",
      "torch.Size([5999])\n",
      "torch.Size([10590])\n",
      "torch.Size([4096])\n",
      "torch.Size([4219])\n",
      "torch.Size([4096])\n",
      "torch.Size([11798])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4812])\n",
      "torch.Size([4096])\n",
      "torch.Size([14581])\n",
      "torch.Size([4096])\n",
      "torch.Size([11171])\n",
      "torch.Size([14210])\n",
      "torch.Size([6281])\n",
      "torch.Size([7551])\n",
      "torch.Size([7523])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([19797])\n",
      "torch.Size([14207])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([16470])\n",
      "torch.Size([6515])\n",
      "torch.Size([4625])\n",
      "torch.Size([11731])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([15591])\n",
      "torch.Size([7327])\n",
      "torch.Size([4096])\n",
      "torch.Size([26079])\n",
      "torch.Size([9061])\n",
      "torch.Size([21696])\n",
      "torch.Size([17716])\n",
      "torch.Size([10446])\n",
      "torch.Size([7547])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([10347])\n",
      "torch.Size([8622])\n",
      "torch.Size([4096])\n",
      "torch.Size([5147])\n",
      "torch.Size([29862])\n",
      "torch.Size([4096])\n",
      "torch.Size([9015])\n",
      "torch.Size([11858])\n",
      "torch.Size([26230])\n",
      "torch.Size([4096])\n",
      "torch.Size([8764])\n",
      "torch.Size([29522])\n",
      "torch.Size([57704])\n",
      "torch.Size([17602])\n",
      "torch.Size([16550])\n",
      "torch.Size([6029])\n",
      "torch.Size([12935])\n",
      "torch.Size([15217])\n",
      "torch.Size([4096])\n",
      "torch.Size([9152])\n",
      "torch.Size([5866])\n",
      "torch.Size([4096])\n",
      "torch.Size([4537])\n",
      "torch.Size([8549])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5535])\n",
      "torch.Size([15934])\n",
      "torch.Size([4096])\n",
      "torch.Size([5450])\n",
      "torch.Size([6257])\n",
      "torch.Size([25789])\n",
      "torch.Size([8451])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([18498])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7722])\n",
      "torch.Size([12534])\n",
      "torch.Size([8851])\n",
      "torch.Size([4096])\n",
      "torch.Size([13643])\n",
      "torch.Size([4096])\n",
      "torch.Size([8977])\n",
      "torch.Size([20529])\n",
      "torch.Size([4210])\n",
      "torch.Size([6051])\n",
      "torch.Size([15000])\n",
      "torch.Size([23018])\n",
      "torch.Size([4096])\n",
      "torch.Size([15247])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([12247])\n",
      "torch.Size([5449])\n",
      "torch.Size([4204])\n",
      "torch.Size([26029])\n",
      "torch.Size([4096])\n",
      "torch.Size([9127])\n",
      "torch.Size([4096])\n",
      "torch.Size([18587])\n",
      "torch.Size([13455])\n",
      "torch.Size([4415])\n",
      "torch.Size([7000])\n",
      "torch.Size([5755])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6247])\n",
      "torch.Size([5661])\n",
      "torch.Size([4600])\n",
      "torch.Size([5039])\n",
      "torch.Size([4677])\n",
      "torch.Size([4096])\n",
      "torch.Size([9227])\n",
      "torch.Size([15034])\n",
      "torch.Size([6437])\n",
      "torch.Size([18594])\n",
      "torch.Size([4096])\n",
      "torch.Size([16799])\n",
      "torch.Size([7456])\n",
      "torch.Size([4096])\n",
      "torch.Size([63018])\n",
      "torch.Size([6503])\n",
      "torch.Size([4096])\n",
      "torch.Size([6796])\n",
      "torch.Size([21569])\n",
      "torch.Size([5958])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5768])\n",
      "torch.Size([4096])\n",
      "torch.Size([6183])\n",
      "torch.Size([4096])\n",
      "torch.Size([10899])\n",
      "torch.Size([16389])\n",
      "torch.Size([4096])\n",
      "torch.Size([25249])\n",
      "torch.Size([19283])\n",
      "torch.Size([5031])\n",
      "torch.Size([4840])\n",
      "torch.Size([5059])\n",
      "torch.Size([4096])\n",
      "torch.Size([15671])\n",
      "torch.Size([4096])\n",
      "torch.Size([4833])\n",
      "torch.Size([9869])\n",
      "torch.Size([21111])\n",
      "torch.Size([16375])\n",
      "torch.Size([7770])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([13341])\n",
      "torch.Size([7904])\n",
      "torch.Size([4096])\n",
      "torch.Size([17533])\n",
      "torch.Size([5294])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([17607])\n",
      "torch.Size([4096])\n",
      "torch.Size([29812])\n",
      "torch.Size([38907])\n",
      "torch.Size([4096])\n",
      "torch.Size([7903])\n",
      "torch.Size([11634])\n",
      "torch.Size([31514])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4720])\n",
      "torch.Size([12912])\n",
      "torch.Size([9350])\n",
      "torch.Size([6021])\n",
      "torch.Size([4096])\n",
      "torch.Size([6152])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([17466])\n",
      "torch.Size([6656])\n",
      "torch.Size([28357])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7815])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6064])\n",
      "torch.Size([4096])\n",
      "torch.Size([16253])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5998])\n",
      "torch.Size([22034])\n",
      "torch.Size([10091])\n",
      "torch.Size([4096])\n",
      "torch.Size([14669])\n",
      "torch.Size([4096])\n",
      "torch.Size([8278])\n",
      "torch.Size([24505])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4927])\n",
      "torch.Size([13162])\n",
      "torch.Size([4199])\n",
      "torch.Size([4096])\n",
      "torch.Size([14596])\n",
      "torch.Size([4673])\n",
      "torch.Size([5668])\n",
      "torch.Size([15868])\n",
      "torch.Size([6526])\n",
      "torch.Size([12892])\n",
      "torch.Size([4096])\n",
      "torch.Size([7784])\n",
      "torch.Size([4096])\n",
      "torch.Size([6681])\n",
      "torch.Size([6578])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([16547])\n",
      "torch.Size([6224])\n",
      "torch.Size([7962])\n",
      "torch.Size([5766])\n",
      "torch.Size([4096])\n",
      "torch.Size([6152])\n",
      "torch.Size([4096])\n",
      "torch.Size([22399])\n",
      "torch.Size([5856])\n",
      "torch.Size([4096])\n",
      "torch.Size([5441])\n",
      "torch.Size([11173])\n",
      "torch.Size([4096])\n",
      "torch.Size([5752])\n",
      "torch.Size([7826])\n",
      "torch.Size([4165])\n",
      "torch.Size([5153])\n",
      "torch.Size([5143])\n",
      "torch.Size([4096])\n",
      "torch.Size([5454])\n",
      "torch.Size([13102])\n",
      "torch.Size([7369])\n",
      "torch.Size([4096])\n",
      "torch.Size([17277])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4979])\n",
      "torch.Size([5546])\n",
      "torch.Size([4096])\n",
      "torch.Size([6927])\n",
      "torch.Size([8544])\n",
      "torch.Size([11453])\n",
      "torch.Size([5841])\n",
      "torch.Size([4096])\n",
      "torch.Size([7069])\n",
      "torch.Size([14329])\n",
      "torch.Size([4096])\n",
      "torch.Size([14837])\n",
      "torch.Size([17750])\n",
      "torch.Size([44011])\n",
      "torch.Size([13276])\n",
      "torch.Size([5073])\n",
      "torch.Size([8222])\n",
      "torch.Size([29736])\n",
      "torch.Size([11266])\n",
      "torch.Size([4096])\n",
      "torch.Size([6017])\n",
      "torch.Size([6814])\n",
      "torch.Size([4096])\n",
      "torch.Size([5526])\n",
      "torch.Size([4096])\n",
      "torch.Size([5832])\n",
      "torch.Size([6545])\n",
      "torch.Size([14519])\n",
      "torch.Size([4096])\n",
      "torch.Size([9259])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([18219])\n",
      "torch.Size([13265])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([12063])\n",
      "torch.Size([6063])\n",
      "torch.Size([14768])\n",
      "torch.Size([12896])\n",
      "torch.Size([12080])\n",
      "torch.Size([17864])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4510])\n",
      "torch.Size([4798])\n",
      "torch.Size([24310])\n",
      "torch.Size([11293])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5912])\n",
      "torch.Size([49678])\n",
      "torch.Size([5140])\n",
      "torch.Size([5067])\n",
      "torch.Size([7725])\n",
      "torch.Size([4096])\n",
      "torch.Size([5816])\n",
      "torch.Size([5620])\n",
      "torch.Size([15851])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([9214])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8173])\n",
      "torch.Size([24814])\n",
      "torch.Size([12333])\n",
      "torch.Size([6859])\n",
      "torch.Size([4096])\n",
      "torch.Size([8020])\n",
      "torch.Size([5431])\n",
      "torch.Size([4109])\n",
      "torch.Size([7689])\n",
      "torch.Size([11219])\n",
      "torch.Size([8788])\n",
      "torch.Size([14702])\n",
      "torch.Size([6478])\n",
      "torch.Size([5595])\n",
      "torch.Size([17404])\n",
      "torch.Size([15835])\n",
      "torch.Size([5197])\n",
      "torch.Size([6980])\n",
      "torch.Size([4096])\n",
      "torch.Size([16851])\n",
      "torch.Size([22337])\n",
      "torch.Size([13746])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8015])\n",
      "torch.Size([18334])\n",
      "torch.Size([50785])\n",
      "torch.Size([12053])\n",
      "torch.Size([56644])\n",
      "torch.Size([18703])\n",
      "torch.Size([4194])\n",
      "torch.Size([4096])\n",
      "torch.Size([7418])\n",
      "torch.Size([4096])\n",
      "torch.Size([4559])\n",
      "torch.Size([6386])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([20524])\n",
      "torch.Size([11373])\n",
      "torch.Size([5598])\n",
      "torch.Size([12419])\n",
      "torch.Size([13692])\n",
      "torch.Size([4096])\n",
      "torch.Size([24802])\n",
      "torch.Size([8644])\n",
      "torch.Size([6786])\n",
      "torch.Size([4096])\n",
      "torch.Size([5651])\n",
      "torch.Size([4096])\n",
      "torch.Size([9323])\n",
      "torch.Size([13892])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8582])\n",
      "torch.Size([7233])\n",
      "torch.Size([4096])\n",
      "torch.Size([16242])\n",
      "torch.Size([11238])\n",
      "torch.Size([4096])\n",
      "torch.Size([17948])\n",
      "torch.Size([4096])\n",
      "torch.Size([37942])\n",
      "torch.Size([4168])\n",
      "torch.Size([5851])\n",
      "torch.Size([6028])\n",
      "torch.Size([4096])\n",
      "torch.Size([4511])\n",
      "torch.Size([4096])\n",
      "torch.Size([13673])\n",
      "torch.Size([4096])\n",
      "torch.Size([7909])\n",
      "torch.Size([6426])\n",
      "torch.Size([10028])\n",
      "torch.Size([4096])\n",
      "torch.Size([16780])\n",
      "torch.Size([9077])\n",
      "torch.Size([5850])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([22687])\n",
      "torch.Size([4096])\n",
      "torch.Size([27038])\n",
      "torch.Size([4096])\n",
      "torch.Size([6700])\n",
      "torch.Size([11325])\n",
      "torch.Size([25906])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([17791])\n",
      "torch.Size([6508])\n",
      "torch.Size([4096])\n",
      "torch.Size([6173])\n",
      "torch.Size([4432])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5700])\n",
      "torch.Size([4096])\n",
      "torch.Size([11037])\n",
      "torch.Size([8353])\n",
      "torch.Size([5418])\n",
      "torch.Size([6523])\n",
      "torch.Size([5883])\n",
      "torch.Size([15270])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8210])\n",
      "torch.Size([8325])\n",
      "torch.Size([4096])\n",
      "torch.Size([10054])\n",
      "torch.Size([4096])\n",
      "torch.Size([16466])\n",
      "torch.Size([8704])\n",
      "torch.Size([4349])\n",
      "torch.Size([7883])\n",
      "torch.Size([4096])\n",
      "torch.Size([10278])\n",
      "torch.Size([8906])\n",
      "torch.Size([14560])\n",
      "torch.Size([13758])\n",
      "torch.Size([30904])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([18533])\n",
      "torch.Size([4096])\n",
      "torch.Size([8168])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4466])\n",
      "torch.Size([7376])\n",
      "torch.Size([4096])\n",
      "torch.Size([19676])\n",
      "torch.Size([20089])\n",
      "torch.Size([8477])\n",
      "torch.Size([4096])\n",
      "torch.Size([5130])\n",
      "torch.Size([10223])\n",
      "torch.Size([22203])\n",
      "torch.Size([6026])\n",
      "torch.Size([8966])\n",
      "torch.Size([4096])\n",
      "torch.Size([14019])\n",
      "torch.Size([20065])\n",
      "torch.Size([26991])\n",
      "torch.Size([9150])\n",
      "torch.Size([4096])\n",
      "torch.Size([6424])\n",
      "torch.Size([8923])\n",
      "torch.Size([25811])\n",
      "torch.Size([10888])\n",
      "torch.Size([14774])\n",
      "torch.Size([4096])\n",
      "torch.Size([6588])\n",
      "torch.Size([10383])\n",
      "torch.Size([7436])\n",
      "torch.Size([8842])\n",
      "torch.Size([6964])\n",
      "torch.Size([22343])\n",
      "torch.Size([4096])\n",
      "torch.Size([4839])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([13112])\n",
      "torch.Size([5293])\n",
      "torch.Size([4096])\n",
      "torch.Size([53063])\n",
      "torch.Size([4096])\n",
      "torch.Size([4776])\n",
      "torch.Size([6783])\n",
      "torch.Size([10489])\n",
      "torch.Size([4096])\n",
      "torch.Size([5907])\n",
      "torch.Size([4096])\n",
      "torch.Size([5255])\n",
      "torch.Size([4096])\n",
      "torch.Size([13353])\n",
      "torch.Size([9075])\n",
      "torch.Size([4096])\n",
      "torch.Size([11431])\n",
      "torch.Size([4096])\n",
      "torch.Size([10980])\n",
      "torch.Size([6500])\n",
      "torch.Size([12344])\n",
      "torch.Size([15409])\n",
      "torch.Size([4178])\n",
      "torch.Size([5468])\n",
      "torch.Size([7403])\n",
      "torch.Size([13202])\n",
      "torch.Size([4481])\n",
      "torch.Size([5039])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([9424])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([16403])\n",
      "torch.Size([18950])\n",
      "torch.Size([13973])\n",
      "torch.Size([14603])\n",
      "torch.Size([8044])\n",
      "torch.Size([4873])\n",
      "torch.Size([14594])\n",
      "torch.Size([4270])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([10785])\n",
      "torch.Size([4110])\n",
      "torch.Size([8859])\n",
      "torch.Size([4096])\n",
      "torch.Size([5371])\n",
      "torch.Size([4776])\n",
      "torch.Size([25480])\n",
      "torch.Size([4096])\n",
      "torch.Size([9592])\n",
      "torch.Size([18789])\n",
      "torch.Size([4096])\n",
      "torch.Size([8534])\n",
      "torch.Size([16925])\n",
      "torch.Size([4785])\n",
      "torch.Size([10063])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4680])\n",
      "torch.Size([4096])\n",
      "torch.Size([4336])\n",
      "torch.Size([14705])\n",
      "torch.Size([14868])\n",
      "torch.Size([4096])\n",
      "torch.Size([5582])\n",
      "torch.Size([6090])\n",
      "torch.Size([5030])\n",
      "torch.Size([11989])\n",
      "torch.Size([4096])\n",
      "torch.Size([33716])\n",
      "torch.Size([18677])\n",
      "torch.Size([12086])\n",
      "torch.Size([4797])\n",
      "torch.Size([12009])\n",
      "torch.Size([4096])\n",
      "torch.Size([4146])\n",
      "torch.Size([4096])\n",
      "torch.Size([6025])\n",
      "torch.Size([28143])\n",
      "torch.Size([4096])\n",
      "torch.Size([21442])\n",
      "torch.Size([15593])\n",
      "torch.Size([4096])\n",
      "torch.Size([7530])\n",
      "torch.Size([4096])\n",
      "torch.Size([15390])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4958])\n",
      "torch.Size([10765])\n",
      "torch.Size([4415])\n",
      "torch.Size([14736])\n",
      "torch.Size([28550])\n",
      "torch.Size([8669])\n",
      "torch.Size([4096])\n",
      "torch.Size([7456])\n",
      "torch.Size([4417])\n",
      "torch.Size([5965])\n",
      "torch.Size([33569])\n",
      "torch.Size([4658])\n",
      "torch.Size([4096])\n",
      "torch.Size([4994])\n",
      "torch.Size([7822])\n",
      "torch.Size([4662])\n",
      "torch.Size([25184])\n",
      "torch.Size([11241])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6487])\n",
      "torch.Size([6844])\n",
      "torch.Size([4096])\n",
      "torch.Size([4127])\n",
      "torch.Size([4096])\n",
      "torch.Size([7297])\n",
      "torch.Size([19683])\n",
      "torch.Size([4096])\n",
      "torch.Size([14381])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4533])\n",
      "torch.Size([6933])\n",
      "torch.Size([7505])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6069])\n",
      "torch.Size([8549])\n",
      "torch.Size([27282])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([10383])\n",
      "torch.Size([4096])\n",
      "torch.Size([19628])\n",
      "torch.Size([4096])\n",
      "torch.Size([13404])\n",
      "torch.Size([4096])\n",
      "torch.Size([4408])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([23700])\n",
      "torch.Size([11037])\n",
      "torch.Size([7993])\n",
      "torch.Size([19807])\n",
      "torch.Size([13630])\n",
      "torch.Size([13513])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5121])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([15264])\n",
      "torch.Size([4096])\n",
      "torch.Size([8545])\n",
      "torch.Size([7515])\n",
      "torch.Size([4096])\n",
      "torch.Size([4339])\n",
      "torch.Size([4096])\n",
      "torch.Size([6200])\n",
      "torch.Size([8618])\n",
      "torch.Size([4096])\n",
      "torch.Size([10542])\n",
      "torch.Size([4096])\n",
      "torch.Size([8665])\n",
      "torch.Size([11470])\n",
      "torch.Size([8527])\n",
      "torch.Size([5414])\n",
      "torch.Size([8774])\n",
      "torch.Size([10493])\n",
      "torch.Size([20393])\n",
      "torch.Size([4991])\n",
      "torch.Size([10247])\n",
      "torch.Size([18730])\n",
      "torch.Size([9487])\n",
      "torch.Size([4096])\n",
      "torch.Size([11251])\n",
      "torch.Size([14095])\n",
      "torch.Size([24580])\n",
      "torch.Size([4168])\n",
      "torch.Size([10369])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([16749])\n",
      "torch.Size([4410])\n",
      "torch.Size([4497])\n",
      "torch.Size([4768])\n",
      "torch.Size([6308])\n",
      "torch.Size([6879])\n",
      "torch.Size([9183])\n",
      "torch.Size([5628])\n",
      "torch.Size([4877])\n",
      "torch.Size([29863])\n",
      "torch.Size([4096])\n",
      "torch.Size([4100])\n",
      "torch.Size([4096])\n",
      "torch.Size([20029])\n",
      "torch.Size([16124])\n",
      "torch.Size([4096])\n",
      "torch.Size([4849])\n",
      "torch.Size([5448])\n",
      "torch.Size([10366])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7683])\n",
      "torch.Size([12778])\n",
      "torch.Size([24424])\n",
      "torch.Size([10996])\n",
      "torch.Size([4096])\n",
      "torch.Size([4932])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5479])\n",
      "torch.Size([30118])\n",
      "torch.Size([6227])\n",
      "torch.Size([10705])\n",
      "torch.Size([4096])\n",
      "torch.Size([4649])\n",
      "torch.Size([18669])\n",
      "torch.Size([8636])\n",
      "torch.Size([5030])\n",
      "torch.Size([10401])\n",
      "torch.Size([5090])\n",
      "torch.Size([9454])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8710])\n",
      "torch.Size([6106])\n",
      "torch.Size([17148])\n",
      "torch.Size([26160])\n",
      "torch.Size([4096])\n",
      "torch.Size([4523])\n",
      "torch.Size([6789])\n",
      "torch.Size([4096])\n",
      "torch.Size([10573])\n",
      "torch.Size([15045])\n",
      "torch.Size([12858])\n",
      "torch.Size([4096])\n",
      "torch.Size([4254])\n",
      "torch.Size([17066])\n",
      "torch.Size([4096])\n",
      "torch.Size([6015])\n",
      "torch.Size([8804])\n",
      "torch.Size([36540])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8566])\n",
      "torch.Size([14776])\n",
      "torch.Size([5945])\n",
      "torch.Size([8146])\n",
      "torch.Size([4096])\n",
      "torch.Size([6269])\n",
      "torch.Size([4096])\n",
      "torch.Size([5892])\n",
      "torch.Size([49950])\n",
      "torch.Size([25509])\n",
      "torch.Size([14637])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8346])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5917])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([31744])\n",
      "torch.Size([4096])\n",
      "torch.Size([14955])\n",
      "torch.Size([7014])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([16083])\n",
      "torch.Size([16522])\n",
      "torch.Size([17468])\n",
      "torch.Size([4096])\n",
      "torch.Size([4764])\n",
      "torch.Size([31188])\n",
      "torch.Size([4096])\n",
      "torch.Size([6802])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7510])\n",
      "torch.Size([4096])\n",
      "torch.Size([5320])\n",
      "torch.Size([6599])\n",
      "torch.Size([12661])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([9691])\n",
      "torch.Size([4096])\n",
      "torch.Size([13324])\n",
      "torch.Size([7104])\n",
      "torch.Size([13549])\n",
      "torch.Size([12051])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7798])\n",
      "torch.Size([6678])\n",
      "torch.Size([20613])\n",
      "torch.Size([15840])\n",
      "torch.Size([18780])\n",
      "torch.Size([27189])\n",
      "torch.Size([4936])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([19628])\n",
      "torch.Size([6795])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4797])\n",
      "torch.Size([4096])\n",
      "torch.Size([20434])\n",
      "torch.Size([10765])\n",
      "torch.Size([4096])\n",
      "torch.Size([6017])\n",
      "torch.Size([14729])\n",
      "torch.Size([4096])\n",
      "torch.Size([15456])\n",
      "torch.Size([19251])\n",
      "torch.Size([11303])\n",
      "torch.Size([8459])\n",
      "torch.Size([5164])\n",
      "torch.Size([11260])\n",
      "torch.Size([11672])\n",
      "torch.Size([8437])\n",
      "torch.Size([4096])\n",
      "torch.Size([31166])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([11617])\n",
      "torch.Size([7087])\n",
      "torch.Size([4096])\n",
      "torch.Size([6710])\n",
      "torch.Size([4096])\n",
      "torch.Size([8353])\n",
      "torch.Size([4695])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4604])\n",
      "torch.Size([5926])\n",
      "torch.Size([5036])\n",
      "torch.Size([20869])\n",
      "torch.Size([26472])\n",
      "torch.Size([30453])\n",
      "torch.Size([13901])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([9262])\n",
      "torch.Size([27845])\n",
      "torch.Size([4096])\n",
      "torch.Size([4974])\n",
      "torch.Size([4096])\n",
      "torch.Size([4542])\n",
      "torch.Size([5638])\n",
      "torch.Size([5317])\n",
      "torch.Size([4096])\n",
      "torch.Size([7853])\n",
      "torch.Size([4096])\n",
      "torch.Size([11887])\n",
      "torch.Size([8947])\n",
      "torch.Size([5724])\n",
      "torch.Size([21972])\n",
      "torch.Size([5500])\n",
      "torch.Size([9710])\n",
      "torch.Size([7286])\n",
      "torch.Size([12322])\n",
      "torch.Size([4096])\n",
      "torch.Size([8350])\n",
      "torch.Size([6832])\n",
      "torch.Size([4637])\n",
      "torch.Size([11944])\n",
      "torch.Size([14996])\n",
      "torch.Size([4096])\n",
      "torch.Size([11340])\n",
      "torch.Size([4096])\n",
      "torch.Size([5829])\n",
      "torch.Size([4272])\n",
      "torch.Size([4096])\n",
      "torch.Size([4315])\n",
      "torch.Size([20298])\n",
      "torch.Size([8191])\n",
      "torch.Size([4096])\n",
      "torch.Size([12432])\n",
      "torch.Size([9719])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([15645])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([15773])\n",
      "torch.Size([4096])\n",
      "torch.Size([15818])\n",
      "torch.Size([26304])\n",
      "torch.Size([24572])\n",
      "torch.Size([4096])\n",
      "torch.Size([4399])\n",
      "torch.Size([10933])\n",
      "torch.Size([4096])\n",
      "torch.Size([4345])\n",
      "torch.Size([5440])\n",
      "torch.Size([4096])\n",
      "torch.Size([6419])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6476])\n",
      "torch.Size([4096])\n",
      "torch.Size([11601])\n",
      "torch.Size([6422])\n",
      "torch.Size([4343])\n",
      "torch.Size([21801])\n",
      "torch.Size([5874])\n",
      "torch.Size([4096])\n",
      "torch.Size([16090])\n",
      "torch.Size([5479])\n",
      "torch.Size([4339])\n",
      "torch.Size([19390])\n",
      "torch.Size([4096])\n",
      "torch.Size([13470])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([11765])\n",
      "torch.Size([12466])\n",
      "torch.Size([11238])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([13684])\n",
      "torch.Size([4974])\n",
      "torch.Size([5298])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5827])\n",
      "torch.Size([15938])\n",
      "torch.Size([16567])\n",
      "torch.Size([4096])\n",
      "torch.Size([8332])\n",
      "torch.Size([4096])\n",
      "torch.Size([4942])\n",
      "torch.Size([4096])\n",
      "torch.Size([13301])\n",
      "torch.Size([10737])\n",
      "torch.Size([4096])\n",
      "torch.Size([6615])\n",
      "torch.Size([32004])\n",
      "torch.Size([9947])\n",
      "torch.Size([9838])\n",
      "torch.Size([10593])\n",
      "torch.Size([14233])\n",
      "torch.Size([4543])\n",
      "torch.Size([4630])\n",
      "torch.Size([8042])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5326])\n",
      "torch.Size([14817])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([28843])\n",
      "torch.Size([4096])\n",
      "torch.Size([14439])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5145])\n",
      "torch.Size([26093])\n",
      "torch.Size([15286])\n",
      "torch.Size([4096])\n",
      "torch.Size([27437])\n",
      "torch.Size([4096])\n",
      "torch.Size([7612])\n",
      "torch.Size([22553])\n",
      "torch.Size([5522])\n",
      "torch.Size([7034])\n",
      "torch.Size([7764])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([16385])\n",
      "torch.Size([4096])\n",
      "torch.Size([8959])\n",
      "torch.Size([14214])\n",
      "torch.Size([11566])\n",
      "torch.Size([4096])\n",
      "torch.Size([13479])\n",
      "torch.Size([6301])\n",
      "torch.Size([7014])\n",
      "torch.Size([4096])\n",
      "torch.Size([6292])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5191])\n",
      "torch.Size([8568])\n",
      "torch.Size([4096])\n",
      "torch.Size([13216])\n",
      "torch.Size([7206])\n",
      "torch.Size([9439])\n",
      "torch.Size([12643])\n",
      "torch.Size([4096])\n",
      "torch.Size([11996])\n",
      "torch.Size([4570])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7997])\n",
      "torch.Size([18057])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7347])\n",
      "torch.Size([4096])\n",
      "torch.Size([5076])\n",
      "torch.Size([4096])\n",
      "torch.Size([11067])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5939])\n",
      "torch.Size([7391])\n",
      "torch.Size([12546])\n",
      "torch.Size([7120])\n",
      "torch.Size([5522])\n",
      "torch.Size([10529])\n",
      "torch.Size([29510])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8389])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([10686])\n",
      "torch.Size([4313])\n",
      "torch.Size([25372])\n",
      "torch.Size([4910])\n",
      "torch.Size([4096])\n",
      "torch.Size([19067])\n",
      "torch.Size([6403])\n",
      "torch.Size([4096])\n",
      "torch.Size([10635])\n",
      "torch.Size([4096])\n",
      "torch.Size([9903])\n",
      "torch.Size([8001])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7397])\n",
      "torch.Size([4096])\n",
      "torch.Size([18196])\n",
      "torch.Size([10041])\n",
      "torch.Size([4096])\n",
      "torch.Size([9810])\n",
      "torch.Size([4096])\n",
      "torch.Size([26062])\n",
      "torch.Size([6326])\n",
      "torch.Size([8191])\n",
      "torch.Size([17471])\n",
      "torch.Size([10974])\n",
      "torch.Size([11128])\n",
      "torch.Size([4096])\n",
      "torch.Size([18781])\n",
      "torch.Size([19777])\n",
      "torch.Size([4096])\n",
      "torch.Size([15947])\n",
      "torch.Size([18799])\n",
      "torch.Size([25570])\n",
      "torch.Size([7870])\n",
      "torch.Size([4096])\n",
      "torch.Size([4944])\n",
      "torch.Size([4096])\n",
      "torch.Size([16508])\n",
      "torch.Size([4096])\n",
      "torch.Size([4260])\n",
      "torch.Size([11630])\n",
      "torch.Size([9580])\n",
      "torch.Size([4096])\n",
      "torch.Size([6091])\n",
      "torch.Size([18040])\n",
      "torch.Size([9638])\n",
      "torch.Size([4096])\n",
      "torch.Size([4201])\n",
      "torch.Size([7639])\n",
      "torch.Size([14511])\n",
      "torch.Size([4096])\n",
      "torch.Size([4359])\n",
      "torch.Size([7216])\n",
      "torch.Size([4096])\n",
      "torch.Size([28099])\n",
      "torch.Size([4783])\n",
      "torch.Size([6108])\n",
      "torch.Size([22336])\n",
      "torch.Size([20909])\n",
      "torch.Size([21113])\n",
      "torch.Size([4266])\n",
      "torch.Size([7817])\n",
      "torch.Size([5919])\n",
      "torch.Size([5212])\n",
      "torch.Size([4096])\n",
      "torch.Size([22758])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5544])\n",
      "torch.Size([4933])\n",
      "torch.Size([11688])\n",
      "torch.Size([6922])\n",
      "torch.Size([4514])\n",
      "torch.Size([4096])\n",
      "torch.Size([8583])\n",
      "torch.Size([18670])\n",
      "torch.Size([7520])\n",
      "torch.Size([5807])\n",
      "torch.Size([15213])\n",
      "torch.Size([15334])\n",
      "torch.Size([4096])\n",
      "torch.Size([4299])\n",
      "torch.Size([4749])\n",
      "torch.Size([8582])\n",
      "torch.Size([5576])\n",
      "torch.Size([6969])\n",
      "torch.Size([4474])\n",
      "torch.Size([4096])\n",
      "torch.Size([4899])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7155])\n",
      "torch.Size([4096])\n",
      "torch.Size([17882])\n",
      "torch.Size([25230])\n",
      "torch.Size([10181])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([12262])\n",
      "torch.Size([4096])\n",
      "torch.Size([8836])\n",
      "torch.Size([11339])\n",
      "torch.Size([11588])\n",
      "torch.Size([4096])\n",
      "torch.Size([5362])\n",
      "torch.Size([10877])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4776])\n",
      "torch.Size([5379])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([17872])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5485])\n",
      "torch.Size([8247])\n",
      "torch.Size([11584])\n",
      "torch.Size([8600])\n",
      "torch.Size([10174])\n",
      "torch.Size([4096])\n",
      "torch.Size([8550])\n",
      "torch.Size([20036])\n",
      "torch.Size([4096])\n",
      "torch.Size([6859])\n",
      "torch.Size([9085])\n",
      "torch.Size([14645])\n",
      "torch.Size([4513])\n",
      "torch.Size([12512])\n",
      "torch.Size([4096])\n",
      "torch.Size([18641])\n",
      "torch.Size([20569])\n",
      "torch.Size([4096])\n",
      "torch.Size([4340])\n",
      "torch.Size([30713])\n",
      "torch.Size([15845])\n",
      "torch.Size([4096])\n",
      "torch.Size([7841])\n",
      "torch.Size([4576])\n",
      "torch.Size([20198])\n",
      "torch.Size([4096])\n",
      "torch.Size([13725])\n",
      "torch.Size([7790])\n",
      "torch.Size([39558])\n",
      "torch.Size([7217])\n",
      "torch.Size([22213])\n",
      "torch.Size([6866])\n",
      "torch.Size([4096])\n",
      "torch.Size([7165])\n",
      "torch.Size([4947])\n",
      "torch.Size([4096])\n",
      "torch.Size([11950])\n",
      "torch.Size([14059])\n",
      "torch.Size([4096])\n",
      "torch.Size([20656])\n",
      "torch.Size([9065])\n",
      "torch.Size([51204])\n",
      "torch.Size([4106])\n",
      "torch.Size([14484])\n",
      "torch.Size([15372])\n",
      "torch.Size([11795])\n",
      "torch.Size([4096])\n",
      "torch.Size([21345])\n",
      "torch.Size([18581])\n",
      "torch.Size([8246])\n",
      "torch.Size([16748])\n",
      "torch.Size([11265])\n",
      "torch.Size([8933])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([20856])\n",
      "torch.Size([19585])\n",
      "torch.Size([4096])\n",
      "torch.Size([9705])\n",
      "torch.Size([18439])\n",
      "torch.Size([4096])\n",
      "torch.Size([8483])\n",
      "torch.Size([5992])\n",
      "torch.Size([5422])\n",
      "torch.Size([4096])\n",
      "torch.Size([5936])\n",
      "torch.Size([9832])\n",
      "torch.Size([12443])\n",
      "torch.Size([4096])\n",
      "torch.Size([36898])\n",
      "torch.Size([4361])\n",
      "torch.Size([5541])\n",
      "torch.Size([19124])\n",
      "torch.Size([4096])\n",
      "torch.Size([6009])\n",
      "torch.Size([4096])\n",
      "torch.Size([4350])\n",
      "torch.Size([5217])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([26721])\n",
      "torch.Size([4586])\n",
      "torch.Size([4096])\n",
      "torch.Size([5358])\n",
      "torch.Size([14527])\n",
      "torch.Size([4096])\n",
      "torch.Size([9785])\n",
      "torch.Size([4096])\n",
      "torch.Size([16627])\n",
      "torch.Size([23823])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6383])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6731])\n",
      "torch.Size([17864])\n",
      "torch.Size([13578])\n",
      "torch.Size([8301])\n",
      "torch.Size([4096])\n",
      "torch.Size([6980])\n",
      "torch.Size([4096])\n",
      "torch.Size([17989])\n",
      "torch.Size([4096])\n",
      "torch.Size([32127])\n",
      "torch.Size([16426])\n",
      "torch.Size([25774])\n",
      "torch.Size([30822])\n",
      "torch.Size([4096])\n",
      "torch.Size([20863])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([11000])\n",
      "torch.Size([6828])\n",
      "torch.Size([23233])\n",
      "torch.Size([5632])\n",
      "torch.Size([10340])\n",
      "torch.Size([4096])\n",
      "torch.Size([20519])\n",
      "torch.Size([39120])\n",
      "torch.Size([13877])\n",
      "torch.Size([4096])\n",
      "torch.Size([7154])\n",
      "torch.Size([8797])\n",
      "torch.Size([10246])\n",
      "torch.Size([4939])\n",
      "torch.Size([5201])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([18162])\n",
      "torch.Size([4096])\n",
      "torch.Size([4726])\n",
      "torch.Size([4578])\n",
      "torch.Size([4096])\n",
      "torch.Size([7987])\n",
      "torch.Size([8368])\n",
      "torch.Size([26269])\n",
      "torch.Size([5417])\n",
      "torch.Size([13006])\n",
      "torch.Size([4096])\n",
      "torch.Size([10442])\n",
      "torch.Size([9964])\n",
      "torch.Size([5037])\n",
      "torch.Size([7490])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([16793])\n",
      "torch.Size([5326])\n",
      "torch.Size([11802])\n",
      "torch.Size([4096])\n",
      "torch.Size([21151])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([9212])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6948])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([28371])\n",
      "torch.Size([4096])\n",
      "torch.Size([6557])\n",
      "torch.Size([8974])\n",
      "torch.Size([4363])\n",
      "torch.Size([4096])\n",
      "torch.Size([17239])\n",
      "torch.Size([4096])\n",
      "torch.Size([26907])\n",
      "torch.Size([6791])\n",
      "torch.Size([5043])\n",
      "torch.Size([9058])\n",
      "torch.Size([13231])\n",
      "torch.Size([27320])\n",
      "torch.Size([4096])\n",
      "torch.Size([7258])\n",
      "torch.Size([9449])\n",
      "torch.Size([4096])\n",
      "torch.Size([21170])\n",
      "torch.Size([14562])\n",
      "torch.Size([4923])\n",
      "torch.Size([17581])\n",
      "torch.Size([4096])\n",
      "torch.Size([8596])\n",
      "torch.Size([12927])\n",
      "torch.Size([25443])\n",
      "torch.Size([4096])\n",
      "torch.Size([9038])\n",
      "torch.Size([50902])\n",
      "torch.Size([29403])\n",
      "torch.Size([17168])\n",
      "torch.Size([15062])\n",
      "torch.Size([4096])\n",
      "torch.Size([4910])\n",
      "torch.Size([4096])\n",
      "torch.Size([17621])\n",
      "torch.Size([19190])\n",
      "torch.Size([4096])\n",
      "torch.Size([9639])\n",
      "torch.Size([4895])\n",
      "torch.Size([18531])\n",
      "torch.Size([6130])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([21953])\n",
      "torch.Size([7180])\n",
      "torch.Size([4096])\n",
      "torch.Size([8816])\n",
      "torch.Size([6261])\n",
      "torch.Size([4096])\n",
      "torch.Size([4976])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([5130])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([26120])\n",
      "torch.Size([6167])\n",
      "torch.Size([4096])\n",
      "torch.Size([19593])\n",
      "torch.Size([15946])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([23185])\n",
      "torch.Size([4096])\n",
      "torch.Size([8164])\n",
      "torch.Size([5780])\n",
      "torch.Size([4096])\n",
      "torch.Size([17901])\n",
      "torch.Size([25238])\n",
      "torch.Size([5707])\n",
      "torch.Size([5037])\n",
      "torch.Size([17781])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([13898])\n",
      "torch.Size([4096])\n",
      "torch.Size([5845])\n",
      "torch.Size([9899])\n",
      "torch.Size([4987])\n",
      "torch.Size([7424])\n",
      "torch.Size([7569])\n",
      "torch.Size([6947])\n",
      "torch.Size([4096])\n",
      "torch.Size([5942])\n",
      "torch.Size([4096])\n",
      "torch.Size([33322])\n",
      "torch.Size([14318])\n",
      "torch.Size([7531])\n",
      "torch.Size([4096])\n",
      "torch.Size([26840])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([13730])\n",
      "torch.Size([79553])\n",
      "torch.Size([4096])\n",
      "torch.Size([5435])\n",
      "torch.Size([9058])\n",
      "torch.Size([26251])\n",
      "torch.Size([4096])\n",
      "torch.Size([68700])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4156])\n",
      "torch.Size([4096])\n",
      "torch.Size([24188])\n",
      "torch.Size([4883])\n",
      "torch.Size([7356])\n",
      "torch.Size([4096])\n",
      "torch.Size([11108])\n",
      "torch.Size([19451])\n",
      "torch.Size([13336])\n",
      "torch.Size([15311])\n",
      "torch.Size([4096])\n",
      "torch.Size([8100])\n",
      "torch.Size([5635])\n",
      "torch.Size([9726])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([14369])\n",
      "torch.Size([6014])\n",
      "torch.Size([18811])\n",
      "torch.Size([4096])\n",
      "torch.Size([23318])\n",
      "torch.Size([23396])\n",
      "torch.Size([4096])\n",
      "torch.Size([10185])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6613])\n",
      "torch.Size([13373])\n",
      "torch.Size([7999])\n",
      "torch.Size([25987])\n",
      "torch.Size([19880])\n",
      "torch.Size([7482])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6934])\n",
      "torch.Size([4096])\n",
      "torch.Size([9838])\n",
      "torch.Size([4096])\n",
      "torch.Size([6203])\n",
      "torch.Size([5741])\n",
      "torch.Size([34599])\n",
      "torch.Size([4096])\n",
      "torch.Size([4464])\n",
      "torch.Size([10248])\n",
      "torch.Size([5446])\n",
      "torch.Size([22409])\n",
      "torch.Size([14346])\n",
      "torch.Size([4096])\n",
      "torch.Size([12408])\n",
      "torch.Size([12809])\n",
      "torch.Size([8512])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7187])\n",
      "torch.Size([4096])\n",
      "torch.Size([21764])\n",
      "torch.Size([29797])\n",
      "torch.Size([24143])\n",
      "torch.Size([4301])\n",
      "torch.Size([5095])\n",
      "torch.Size([4096])\n",
      "torch.Size([28997])\n",
      "torch.Size([18801])\n",
      "torch.Size([9830])\n",
      "torch.Size([6204])\n",
      "torch.Size([8586])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8213])\n",
      "torch.Size([38721])\n",
      "torch.Size([5578])\n",
      "torch.Size([10088])\n",
      "torch.Size([4452])\n",
      "torch.Size([6681])\n",
      "torch.Size([11680])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([25402])\n",
      "torch.Size([6353])\n",
      "torch.Size([4096])\n",
      "torch.Size([4975])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([9079])\n",
      "torch.Size([4510])\n",
      "torch.Size([6603])\n",
      "torch.Size([4622])\n",
      "torch.Size([27426])\n",
      "torch.Size([22387])\n",
      "torch.Size([25151])\n",
      "torch.Size([4096])\n",
      "torch.Size([8852])\n",
      "torch.Size([4096])\n",
      "torch.Size([6168])\n",
      "torch.Size([12876])\n",
      "torch.Size([25863])\n",
      "torch.Size([55386])\n",
      "torch.Size([4096])\n",
      "torch.Size([4380])\n",
      "torch.Size([15375])\n",
      "torch.Size([9236])\n",
      "torch.Size([13692])\n",
      "torch.Size([16432])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7512])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4792])\n",
      "torch.Size([23400])\n",
      "torch.Size([12683])\n",
      "torch.Size([7981])\n",
      "torch.Size([13941])\n",
      "torch.Size([7193])\n",
      "torch.Size([10065])\n",
      "torch.Size([16287])\n",
      "torch.Size([7467])\n",
      "torch.Size([13755])\n",
      "torch.Size([39263])\n",
      "torch.Size([4096])\n",
      "torch.Size([4516])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([7635])\n",
      "torch.Size([5394])\n",
      "torch.Size([14882])\n",
      "torch.Size([5747])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([6394])\n",
      "torch.Size([5290])\n",
      "torch.Size([9245])\n",
      "torch.Size([16621])\n",
      "torch.Size([6480])\n",
      "torch.Size([16690])\n",
      "torch.Size([23379])\n",
      "torch.Size([4096])\n",
      "torch.Size([6229])\n",
      "torch.Size([8926])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([8694])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([16622])\n",
      "torch.Size([15244])\n",
      "torch.Size([4096])\n",
      "torch.Size([21828])\n",
      "torch.Size([5811])\n",
      "torch.Size([32147])\n",
      "torch.Size([12813])\n",
      "torch.Size([4096])\n",
      "torch.Size([4710])\n",
      "torch.Size([4096])\n",
      "torch.Size([4191])\n",
      "torch.Size([4096])\n",
      "torch.Size([8325])\n",
      "torch.Size([16702])\n",
      "torch.Size([4096])\n",
      "torch.Size([5113])\n",
      "torch.Size([16763])\n",
      "torch.Size([4096])\n",
      "torch.Size([12033])\n",
      "torch.Size([4096])\n",
      "torch.Size([4661])\n",
      "torch.Size([13028])\n",
      "torch.Size([10286])\n",
      "torch.Size([4096])\n",
      "torch.Size([10637])\n",
      "torch.Size([7420])\n",
      "torch.Size([4096])\n",
      "torch.Size([13706])\n",
      "torch.Size([27048])\n",
      "torch.Size([14115])\n",
      "torch.Size([6095])\n",
      "torch.Size([4096])\n",
      "torch.Size([29256])\n",
      "torch.Size([8634])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-25703c18555a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0moutput_all_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             yield self._getitem(\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mformat_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, format_type, format_columns, output_all_columns, format_kwargs)\u001b[0m\n\u001b[1;32m   1232\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         )\n\u001b[0;32m-> 1234\u001b[0;31m         formatted_output = format_table(\n\u001b[0m\u001b[1;32m   1235\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[0;34m(pa_table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mpa_table_to_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mformatted_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table_to_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRowFormat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumnFormat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchFormat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"row\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py\u001b[0m in \u001b[0;36mformat_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursive_tensorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mextract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mextract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes_mapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_types_mapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_series_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._PandasConvertible.to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table._to_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mtable_to_blockmanager\u001b[0;34m(options, table, categories, ignore_metadata, types_mapper)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0m_check_data_column_metadata_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_column_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_table_to_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext_columns_dtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m_deserialize_column_index\u001b[0;34m(block_table, all_columns, column_indexes)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pandas_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m         columns = _pandas_api.pd.MultiIndex.from_tuples(\n\u001b[0m\u001b[1;32m    893\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn_indexes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mnew_meth\u001b[0;34m(self_or_cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_or_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_meth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mfrom_tuples\u001b[0;34m(cls, tuples, sortorder, names)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msortorder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msortorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mfrom_arrays\u001b[0;34m(cls, arrays, sortorder, names)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all arrays must be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize_from_iterables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mfactorize_from_iterables\u001b[0;34m(iterables)\u001b[0m\n\u001b[1;32m   2630\u001b[0m         \u001b[0;31m# For consistency, it should return a list of 2 lists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2631\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactorize_from_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2630\u001b[0m         \u001b[0;31m# For consistency, it should return a list of 2 lists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2631\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactorize_from_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mfactorize_from_iterable\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   2602\u001b[0m         \u001b[0;31m# but only the resulting categories, the order of which is independent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m         \u001b[0;31m# from ordered. Set ordered to False as default. See GH #15457\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m         \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msort\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         uniques, codes = safe_sort(\n\u001b[0m\u001b[1;32m    728\u001b[0m             \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36msafe_sort\u001b[0;34m(values, codes, na_sentinel, assume_unique, verify)\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0;31m# take_1d is faster, but only works for na_sentinels of -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0morder2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m         \u001b[0mnew_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0;31m# check for promotion based on types only (do this first because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0;31m# it's faster than computing a mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1715\u001b[0;31m             \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m                 \u001b[0;31m# check if promotion is actually required based on indexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_promote\u001b[0;34m(dtype, fill_value)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m                 \u001b[0;31m# upcast to prevent overflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0mmst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_scalar_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcan_cast\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for d in  train_dataset:\n",
    "    print (d['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "supported-haiti",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'question', 'annotations'],\n",
       "        num_rows: 307373\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'document', 'question', 'annotations'],\n",
       "        num_rows: 7830\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-buying",
   "metadata": {},
   "source": [
    "# args testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spanish-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "HfArgumentParser,\n",
    "TrainingArguments,\n",
    ")\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_train_args():\n",
    "    parser = argparse.ArgumentParser('Train spanBert model on NQ')\n",
    "\n",
    "    parser.add_argument('--name',\n",
    "                        type=str,\n",
    "                        default='SpanBert',\n",
    "                        help='Train Name which we use to identify the training. Need not be unique.')\n",
    "    parser.add_argument('--train_file_path',\n",
    "                        type=str,\n",
    "                        default='data/train_data.pt',\n",
    "                        help='Path for cached train dataset')\n",
    "    parser.add_argument('--valid_file_path',\n",
    "                        type=str,\n",
    "                        default='data/valid_data.pt',\n",
    "                        help='Path for cached valid dataset')\n",
    "    parser.add_argument('--max_len',\n",
    "                        type=str,\n",
    "                        default=4096,\n",
    "                        help='Max input length for the source text.')\n",
    "    parser.add_argument('--num_epochs',\n",
    "                        type=int,\n",
    "                        default=3,\n",
    "                        help='Number of epochs for which to train. Negative means forever.')\n",
    "    parser.add_argument('--n_gpu',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help='Number of GPU to train the model on.')\n",
    "    parser.add_argument('--save_dir',\n",
    "                        type=str,\n",
    "                        default='./save',\n",
    "                        help='Directory to store the model in (If already exists, use --overwrite_output_dir).')\n",
    "    parser.add_argument('--overwrite_output_dir',\n",
    "                        type=bool,\n",
    "                        default=True,\n",
    "                        help='Overwrite existing model directory')\n",
    "    parser.add_argument('--per_device_train_batch_size',\n",
    "                        type=int,\n",
    "                        default=8)\n",
    "    parser.add_argument('--per_gpu_eval_batch_size',\n",
    "                        type=int,\n",
    "                        default=8)\n",
    "    parser.add_argument('--gradient_accumulation_steps',\n",
    "                        type=int,\n",
    "                        default=16)\n",
    "    parser.add_argument('--learning_rate',\n",
    "                        type=float,\n",
    "                        default=1e-4,\n",
    "                        help='Learning rate for the model')\n",
    "    parser.add_argument('--num_train_epochs',\n",
    "                        type=int,\n",
    "                        default=3,\n",
    "                        help='Number of epochs for which to train. Negative means forever.')\n",
    "    parser.add_argument('--do_train',\n",
    "                        type=bool,\n",
    "                        default=True,\n",
    "                        help='Whether to train the model.')\n",
    "    parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=42,\n",
    "                        help='Seed to reproduce.')\n",
    "    parser.add_argument('--do_eval',\n",
    "                        type=bool,\n",
    "                        default=True,\n",
    "                        help='Whether to evaluate the model after training.')\n",
    "    parser.add_argument('--local_rank',\n",
    "                        type=int,\n",
    "                        default=-1)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "configured-genesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.training_args.TrainingArguments"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incorrect-deadline",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Train spanBert model on NQ [-h]\n",
      "                                  [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "                                  [--tokenizer_name TOKENIZER_NAME]\n",
      "                                  [--cache_dir CACHE_DIR] [--name NAME]\n",
      "                                  [--train_file_path TRAIN_FILE_PATH]\n",
      "                                  [--valid_file_path VALID_FILE_PATH]\n",
      "                                  [--max_len MAX_LEN]\n",
      "                                  [--num_epochs NUM_EPOCHS] [--n_gpu N_GPU]\n",
      "                                  [--save_dir SAVE_DIR]\n",
      "                                  [--overwrite_output_dir OVERWRITE_OUTPUT_DIR]\n",
      "                                  [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                                  [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                                  [--learning_rate LEARNING_RATE]\n",
      "                                  [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                                  [--do_train DO_TRAIN] [--seed SEED]\n",
      "                                  [--do_eval DO_EVAL]\n",
      "                                  [--local_rank LOCAL_RANK]\n",
      "Train spanBert model on NQ: error: unrecognized arguments: -f /home/sri/.local/share/jupyter/runtime/kernel-e6b51a8b-06c8-40b2-a03d-1db11d4860e2.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sri/miniconda3/envs/dl/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from args import get_train_args\n",
    "parser = get_train_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-final",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-motel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
